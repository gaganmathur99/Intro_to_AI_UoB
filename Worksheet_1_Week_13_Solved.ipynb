{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaganmathur99/Intro_to_AI_UoB/blob/main/Worksheet_1_Week_13_Solved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlttMffDWdos"
      },
      "source": [
        "# Introduction\n",
        "This worksheet covers material from week 13. You will write functions to implement evaluation metrics for classification and regression problems, and run cross-validation. You will:\n",
        " - Use library functions from scikit-learn (https://scikit-learn.org/stable/)\n",
        " - Use NumPy and matplotlib\n",
        " - Write and call functions in Python\n",
        " - Gain understanding of the evaluation metrics used.\n",
        "\n",
        "\n",
        "Scikit-learn (https://scikit-learn.org/stable/) is a Python library with a wide range of ML algorithms. We will be using some of these algorithms during this course, but we will also be looking at the principles behind the algorithms in order to understand these rather than simply applying functions from libraries.\n",
        "\n",
        "## If you don't know Python, NumPy, matplotlib\n",
        "Please work through the Introduction to Python worksheet available on Blackboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBF6yyilWdoz"
      },
      "source": [
        "# 0. Preliminaries\n",
        "We firstly import NumPy and matplotlib as we will be using these throughout the worksheet. We use a 'magic' function `%matplotlib inline` to display plots in the worksheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9F5GZx1Wdo0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPWPZuEmWdo1"
      },
      "source": [
        "# 1. Classification\n",
        "In this question you will use a toy dataset from scikit-learn. You will use functions from scikit-learn to load the data, divide it into training and testing sets, and then fit a simple classifier to the training set. You will then write functions to calculate accuracy, precision, and recall. Finally, you will check your functions against the functions from scikit-learn.\n",
        "\n",
        "## Part a) Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw5wWBNvWdo2"
      },
      "outputs": [],
      "source": [
        "# scikit-learn comes with a number of toy datasets \n",
        "# (https://sklearn.org/datasets/index.html#toy-datasets)\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the wine dataset from sklearn. You may want to take a look at \n",
        "# the format of the dataset\n",
        "wine = datasets.load_wine()\n",
        "\n",
        "# Save the datapoints into the variable X and the targets into \n",
        "# the variable y\n",
        "X = wine.data\n",
        "y = wine.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17YF8kcZWdo2"
      },
      "source": [
        "Take a look at the target values in y. What do you notice about these? Why are these suitable for a classification algorithm rather than a regression algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3-MT9IoWdo3"
      },
      "outputs": [],
      "source": [
        "#  Look at the values in y\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZz3xpnSWdo4"
      },
      "source": [
        "## Part b) Divide the data into training and testing sets\n",
        "Use the function `train_test_split` from `sklearn.model_selection` to split out the data and targets into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcxl-QwgWdo4"
      },
      "outputs": [],
      "source": [
        "# We import the function train_test_split from sklearn and use this to \n",
        "# split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# The function returns splits of each array passed in. \n",
        "# The proportion to be used as the training set is given by test_size\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbHpiU_JWdo5"
      },
      "source": [
        "## Part c) Import the k-nearest neighbours classifier and run it on the data\n",
        "Scikit-learn has a huge range of *estimators* that you can use with your dataset. An estimator is any procedure that can be used to fit data and make predictions from it. Here we will import the k-nearest neighbours classifier, instantiate it, run it on our training set, and then use it to generate some predictions. You will learn more about k-nearest neighbours in Week 14. For now, we are simply using it to generate some predictions.\n",
        "\n",
        "The general procedure for using the estimators in scikit-learn is as follows. Every estimator has a method `fit(X, y)` and a method `predict(T)`. \n",
        "\n",
        "1) Import the estimator\n",
        "    e.g. `from sklearn.models import Classifier`\n",
        "    \n",
        "2) Instantiate the estimator to a variable\n",
        "    e.g. `est = Classifier(params)`\n",
        "    \n",
        "3) Fit the estimator to the data\n",
        "    e.g. `est.fit(X, y)`\n",
        "    \n",
        "4) Make a prediction\n",
        "    e.g. `predictions = est.predict(test_data)`\n",
        "    \n",
        "You can see an example of this in the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "P2IViCdEWdo6"
      },
      "outputs": [],
      "source": [
        "# We first import the classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# We instantiate the classifier with 5 neighbours\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# We fit the model using our training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Finally, we generate predictions on the test data\n",
        "ypred_test=knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-mIDUBJWdo6"
      },
      "source": [
        "## Part d) Evaluating the classifier\n",
        "In this section we will implement functions for accuracy, precision and recall, and compare them with the functions given in sklearn (they should give the same results!)\n",
        "\n",
        "The wine dataset has 3 classes. We will write functions to compute the accuracy of the classifer, the macro-averaged precision and the macro-averaged recall.\n",
        "\n",
        "Recall the equations for accuracy, precision, and recall:\n",
        "\n",
        "$$Accuracy = \\frac{\\sum_{i = 1}^n 1(y_i = f(x_i))}{n}$$\n",
        "i.e. the number of correctly classified datapoints as a proportion of all $n$ datapoints\n",
        "\n",
        "$$Precision_c = \\frac{TP_c}{TP_c+FP_c}$$\n",
        "i.e. the precision for class $c$ is the number of true positives for class $c$ as a proportion of the total number of positive predictions for class $c$\n",
        "\n",
        "$$Recall_c = \\frac{TP_c}{TP_c+FN_c}$$\n",
        "i.e. the recall for class $c$ is the number of true positives for class $c$ as a proportion of the total number of actual positives for class $c$\n",
        "\n",
        "The macro-averaged precision and macro-averaged recall are then simply calculated by averaging the precision (or recall) for each class:\n",
        "\n",
        "$$Precision = \\frac{1}{k} \\sum_{i = 1}^k Precision_k, \\quad Recall = \\frac{1}{k} \\sum_{i = 1}^k Recall_k$$\n",
        "\n",
        "We can automatically generate the confusion matrix for our data using the function `confusion_matrix` from `sklearn.metrics`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmTR36MfWdo7"
      },
      "outputs": [],
      "source": [
        "# Import the function confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Build the confusion matrix from the target test set y_test \n",
        "# and our predicted values ypred_test\n",
        "cm = confusion_matrix(y_test, ypred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQ4QW59Wdo7"
      },
      "source": [
        "Take a look at the confusion matrix. What should its dimensions be? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XINf-MbNWdo8"
      },
      "outputs": [],
      "source": [
        "# Look at the confusion matrix cm\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rhToGzIWdo8"
      },
      "source": [
        "Write a function `my_accuracy` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMhvHe_8Wdo8"
      },
      "outputs": [],
      "source": [
        "def my_accuracy(y, pred):\n",
        "    acc=None\n",
        "    ##TODO###\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlpv6QUbWdo8"
      },
      "source": [
        "Write a function `my_recall_macro` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns recall. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTY6hpcaWdo9"
      },
      "outputs": [],
      "source": [
        "def my_recall_macro(y, pred):\n",
        "    recall = None\n",
        "    cm = confusion_matrix(y, pred)\n",
        "    ##TODO###\n",
        "    return recall      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkOJZAEWdo9"
      },
      "source": [
        "Write a function `my_precision_macro` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns precision. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE_6VL0-Wdo9"
      },
      "outputs": [],
      "source": [
        "def my_precision_macro(y, pred):\n",
        "    pass\n",
        "    ##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZprBMwvUWdo9"
      },
      "source": [
        "Check that your functions match those in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6onwjTFWdo9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "my_accuracy(y_test, ypred_test) == accuracy_score(y_test, ypred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toHP146pWdo-"
      },
      "outputs": [],
      "source": [
        "my_recall_macro(y_test, ypred_test)== \\\n",
        "    recall_score(y_test, ypred_test, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZtuRebAWdo-"
      },
      "outputs": [],
      "source": [
        "my_precision_macro(y_test, ypred_test)== \\\n",
        "    precision_score(y_test, ypred_test, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110FElr3Wdo-"
      },
      "source": [
        "# 2. Regression\n",
        "\n",
        "In this question we will use sklearn to fit a linear model to some artificial data. You will then implement a function to calculate the mean squared error and a function to calculate $r^2$.\n",
        "\n",
        "## Part a) Create an artificial dataset\n",
        "We start off by creating some artificial data.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhWv5Y_7Wdo-"
      },
      "outputs": [],
      "source": [
        "# We set up a random number generator (rng), seeded with a number \n",
        "# (in this case 10). Using a seed means we can generate the same \n",
        "# sequence of pseudorandom numbers, and so we can check results easily.\n",
        "rng = np.random.default_rng(10) \n",
        "\n",
        "# a and b are coefficients for the line\n",
        "a = 2\n",
        "b = -1\n",
        "\n",
        "# Use np.linspace to generate datapoints from 0 to 10 spaced at 0.1\n",
        "X = np.linspace(0, 10, 101) \n",
        "\n",
        "# Create data Y using the equation for a line\n",
        "Y = a*X + b\n",
        "\n",
        "# Adds Gaussian noise with mean 0 and standard deviation 3 to the data\n",
        "Y = Y + 3*rng.standard_normal(len(Y)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGLTzcQrWdo_"
      },
      "source": [
        "## Part b) Plot the data on named axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pH2du_EWdo_"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots() # Generates a named figure (fig) and axes (ax)\n",
        "ax.scatter(X, Y, label = 'Data') # Scatterplot of the data on the ax.\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfwaFCw_Wdo_"
      },
      "source": [
        "## Part c) Split the data into training and testing sets\n",
        "Split the data X and targets Y into training and testing sets using the function `train_test_split`. Set the proportion of the dataset to use as test data to 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ejA-8G9Wdo_"
      },
      "outputs": [],
      "source": [
        "# Write your answer here\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC3pYediWdo_"
      },
      "source": [
        "## Part d) Instantiate and fit the estimator\n",
        "Import the estimator `LinearRegression` from `sklearn.linear_model` and instantiate it. No parameters are needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PytNQUzuWdo_"
      },
      "outputs": [],
      "source": [
        "# Write your answer here\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdOFQXMNWdo_"
      },
      "source": [
        "You will need to reshape the data (but not the targets) before passing it to this model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGzWx8SFWdpA"
      },
      "outputs": [],
      "source": [
        "# This reshapes the data to have 1 column and however many rows make \n",
        "# sense for the data\n",
        "Xtrain = Xtrain.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJsotIciWdpA"
      },
      "source": [
        "Call the method `fit` with your training data to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14wyPOU6WdpA"
      },
      "outputs": [],
      "source": [
        "# Write your answer here\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lwxJ7pYWdpA"
      },
      "source": [
        "The coefficient(s) and intercept for the trained model are called `coef_` and `intercept_`. Are these what you would expect? Are they similar to the values of `a` and `b` that we used to generate the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1K5v5lDWdpA"
      },
      "outputs": [],
      "source": [
        "# Look at coef_ and intercept_\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWuXzvjFWdpA"
      },
      "source": [
        "Using the values stored in `coef_` and `intercept_`, plot a line representing the fitted model on the same axes (`ax`) as before. Type `fig` to view the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rpK-htRWdpB"
      },
      "outputs": [],
      "source": [
        "# Write your answer here \n",
        "##TODO###\n",
        "\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlXP53aCWdpB"
      },
      "source": [
        "Reshape the test data to have one column and then call `predict` on the regression model to get the predicted y values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q-VIkdZWdpB"
      },
      "outputs": [],
      "source": [
        "Xtest = Xtest.reshape(-1, 1)\n",
        "# Call predict on the regression model and assign the output to \n",
        "# a variable ypred\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpDgXLdCWdpB"
      },
      "source": [
        "## Part e) Calculating mean squared error\n",
        "The equation for the mean squared error is \n",
        "$$ MSE = \\frac{\\sum_{i = 1}^n (y_i - f(\\vec{x}_i))^2}{n} $$\n",
        "Write a function that takes in arrays for the target values `y` and the predicted values `pred` and returns the mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDiGIefCWdpB"
      },
      "outputs": [],
      "source": [
        "# Fill in the body of the function\n",
        "def MSE(y, pred):\n",
        "    pass\n",
        "    ##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6lraiZZWdpB"
      },
      "source": [
        "Check your function against the function `mean_squared_error` from `sklearn.metrics`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn8DcUYcWdpB"
      },
      "outputs": [],
      "source": [
        "# Write your answer here\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU57LvHoWdpC"
      },
      "source": [
        "## Part f) Calculating R^2\n",
        "Recall from the lecture that $R^2$ is defined as\n",
        "\n",
        "$$R^2(y, \\hat{y})) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
        "\n",
        "This represents the proportion of the variance of y that is explained by the independent variables in the model. It is essentially a comparison of the fitted model with the mean of the target values ($\\bar{y}$). Implement your own function to calculate $R^2$ and then compare it with `r2_score` from `sklearn.metrics`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK8aQsyaWdpC"
      },
      "outputs": [],
      "source": [
        "# Fill in the body of the function\n",
        "def rsq(y, pred):\n",
        "    pass\n",
        "    ##TODO###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQZU5BHcWdpC"
      },
      "outputs": [],
      "source": [
        "# Compare with sklearn.metrics.r2_score\n",
        "##TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZW7X5b6WdpC"
      },
      "source": [
        "# 3. Using cross-validation for model selection\n",
        "\n",
        "In this section we will fit a series of classifiers with different parameter settings, and use k-fold cross validation to select the most appropriate parameter values. We will use a *decision tree* classifier. We will cover this classifier in more detail in future lectures. For now, all you need to know is that a decision tree has a parameter `max_depth` that says how many layers the tree may have. More layers means that the tree can divide up the feature space into finer classes, but this could come at the expense of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo0JukA0WdpC"
      },
      "source": [
        "## Part a) Creating an artificial dataset\n",
        "We first of all create an artificial dataset with 2000 samples, 10 features, and 4 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06XEXjRRWdpD"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples = 2000, n_features = 10, \\\n",
        "                           n_classes=4, n_informative = 3, random_state=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe1aM4l0WdpD"
      },
      "source": [
        "Use the function `train_test_split` to separate out 0.2 of your dataset. This will be kept aside as our held-out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcTabH8wWdpD"
      },
      "outputs": [],
      "source": [
        "# Write your answer here. \n",
        "# Assign the splits to variables Xtr, Xtest, ytr, ytest\n",
        "# Xtr, Xtest, ytr, ytest = ###TODO###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY5eX4axWdpD"
      },
      "source": [
        "## Part b) Setting up k-fold cross validation\n",
        "We import the function `KFold` from `sklearn.model_selection`, and instantiate it with 10 folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiHtlNhFWdpD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, random_state=63, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZHUWSzGWdpD"
      },
      "source": [
        "## Part c) Running cross-validation for different parameter settings\n",
        "We import the decision tree classifier and run it over depths from 1 to `max_d`. For each depth we run cross-validation over the training set. Write the missing code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hte4umipWdpE"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "max_d = 20\n",
        "#Set up variables to store training and validation accuracies. \n",
        "train_accuracies = [[] for _ in range(max_d)]\n",
        "val_accuracies = [[] for _ in range(max_d)]\n",
        "\n",
        "# Loop over depths\n",
        "for d in range(max_d):\n",
        "    #Instantiate the DecisionTreeClassifier here. \n",
        "    #Set the parameter max_depth to d+1\n",
        "    #We add 1 because d ranges from 0 to max_d-1, \n",
        "    #but we want depths from 1 to max_d\n",
        "    ###TODO###\n",
        "    \n",
        "    #Loop over cross-validation splits. \n",
        "    #Note that we perform cross validation on our training data Xtr.\n",
        "    #We keep our testing data Xtest aside\n",
        "    for train_index, val_index in kf.split(Xtr):\n",
        "        Xtrain, Xval = Xtr[train_index], Xtr[val_index]\n",
        "        ytrain, yval = ytr[train_index], ytr[val_index]\n",
        "        \n",
        "        #Fit the classifier on Xtrain and Ytrain\n",
        "        ###TODO###\n",
        "        \n",
        "        #Make predictions on Xtrain and assign to a variable pred_train\n",
        "        ###TODO###\n",
        "        \n",
        "        #Make predictions on Xval and assign to a variable pred_val\n",
        "        ###TODO###\n",
        "        \n",
        "        #Calculate the accuracy of the predictions on the training set \n",
        "        #and save in the variable train_accuracies\n",
        "        train_accuracies[d].append(accuracy_score(ytrain, pred_train))\n",
        "        \n",
        "        #Do the same for the predictions on the validation set\n",
        "        ###TODO###\n",
        "\n",
        "#Calculate the mean and standard deviation for training \n",
        "#and validation accuracies for each depth across splits \n",
        "train_accuracy_mean = np.mean(train_accuracies, axis=1)\n",
        "# Complete the lines and uncomment\n",
        "# train_accuracy_stdev = ###TODO###\n",
        "# val_accuracy_mean = ###TODO###\n",
        "# val_accuracy_stdev = ###TODO###\n",
        "\n",
        "# The arrays of means and standard deviation should have shape (max_d, ). \n",
        "# The following will generate an error if not.\n",
        "assert(np.shape(train_accuracy_mean)==(max_d,))\n",
        "assert(np.shape(train_accuracy_stdev)==(max_d,))\n",
        "assert(np.shape(val_accuracy_mean)==(max_d,))\n",
        "assert(np.shape(val_accuracy_stdev)==(max_d,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HN0HhkJWdpE"
      },
      "source": [
        "## Plotting results\n",
        "\n",
        "Plot the mean accuracy attained on the training set and on the validation set at each depth. Your figure should look something like the following:\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "What do you notice about these results? Which value of max_depth would you use for your held-out test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9Ry6qF_WdpE"
      },
      "outputs": [],
      "source": [
        "# Write code to plot your results here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzYKRRmtWdpE"
      },
      "source": [
        "## Computing accuracy on the test set\n",
        "Based on your plotted results, decide on the best value of `max_depth` to use in your decision tree classifier. Instantiate a new classifier with your chosen value. Fit the model on the training data. Make a prediction on the held-out test data, and calculate the accuracy on the test data. Is the accuracy what you expected? If not, why not?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6atI-CMWdpE"
      },
      "outputs": [],
      "source": [
        "#Instantiate a new classifier with your chosen value of max_depth\n",
        "\n",
        "#Fit the classifier on the training data\n",
        "\n",
        "#Make a prediction on the test data\n",
        "\n",
        "#Calculate the accuracy on the test data\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "Worksheet 1 Week 13.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}